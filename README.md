# 🧬 HexFormer: Dual-Foundation Architecture Model

> A hybrid architecture for AI symbolic reasoning and semantic emergence,  
> grounded in I-Ching's binary combinatorics and 8-bit operational logic.

---

## 🌌 What is HexFormer?

HexFormer is a dual-layer AI architecture that fuses:

- **2ⁿ-level Symbolic Structures** – e.g., I-Ching's 64 hexagrams, used as semantic state controllers
- **8-bit Stack Representations** – efficient, hardware-friendly computational primitives with bit-level attention

This fusion enables a new generation of AI systems capable of symbolic reasoning, efficient inference, and multimodal generation with traceable semantic flow.

---

## 🧠 Core Philosophy

- **Token is illusion. Bit is essence.**
- Language is not input, but an emergent phenomenon.
- Every bit flip is a semantic ripple; every hexagram, a context switch.

---

## 📐 Architecture Overview

| Layer                    | Role                                   |
|--------------------------|----------------------------------------|
| `2ⁿ Symbolic Layer`      | Semantic control, tempo, state logic   |
| `8-bit Representation`   | Bitwise attention, operational substrate |
| `Transformer Core`       | Swin-style attention & symbolic modulation |
| `Multimodal Decoders`    | Output generation (text, image, audio) |

📄 [Read the full whitepaper](./whitepaper/HexFormer_CN.pdf)

---

## 📚 Project Structure

```bash
├── whitepaper/        # Research papers and design documents
├── docs/              # Blog, index, glossary
├── posts/             # Architecture & theory articles
├── glossary/          # Key term definitions (YaoBit, HexGraph, etc.)
├── symbolic_core/     # 2ⁿ structure control modules (WIP)
├── byte_engine/       # 8-bit attention & reasoning units
├── examples/          # Yao-driven attention flow demos
